---
title: "HarvardX - PH125.9x your own project"
author: "Joe Mark Lippert"
date: "07/12/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, warning=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(devtools)) install.packages("devtools", repos = "http://cran.us.r-project.org")
if(!require(regtools)) install_github("matloff/regtools")
if(!require(partykit)) install.packages("partykit", repos = "http://cran.us.r-project.org")

## abalone ####
#https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/
abalone <- read.csv("abalone.data", header = FALSE, sep = ",")
colnames(abalone) <- c("sex", "length", "diameter", "height", "whole_weight",
                       "shucked_weight", "viscera_weight", "shell_weight",
                       "total_rings")
```

### Introduction

I selected the `abalone` dataset that can be downloaded at [UCI Repository](https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/). 

The objective of this assignment is to predict the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings identified through a microscope. Other measurements, which are easier to obtain, are used to predict the age. The objective is a classification task.

The variables are:

1. sex (Male, Female, Infant)
2. length (measured in mm)
3. diameter (mm)
4. height (mm)
5. whole_weight (grams)
6. shucked_weight (grams)
7. viscera_weight (grams)
8. shell_weight (grams)
9. total_rings (indicates age)

Variables 1 - 8 are the features and, variable 9 is the outcome. The dataset contains 4177 items. In data science speak this number is referred to as *n* and the number of features *p*.

I shall use Machine Learning to predict the outcomes.
Machine learning "deals, directly or indirectly, with estimating the regression function, also called the conditional mean."[Norm Matloff]

### Methods/Analysis  

#### A look in

Let's begin by looking at the data. With any dataset it is always a good idea to take a look around. Identify the features and output, then strategise an approach to preprocessing. 

```{r look, echo=TRUE}
# look at the data type of the variables
glimpse(abalone)

# now let's look at the first 10 items of the dataset
abalone %>% as_tibble()

# then look at the distribution of the classes in the outcome vector
table(abalone$total_rings)

# and again in a histogram
hist(abalone$total_rings, xlab = "total rings")

# lastly, (certainly not least), check for NAs
sum(is.na(abalone))
```

#### First approach  

Let's begin with a common sense approach by predicting age (total_rings) from a single feature.

The feature I will use first is the one that is most closely correlated with total_rings. That feature is shell_weight. See below the correlation calculation.

```{r cor, echo=TRUE}
cor(abalone[2:9])
```

Now assume we foraged an abalone along the False Bay coast in South Africa with shell weight 497 grams (0.497kg), and we want to predict its age. How would we proceed?
We'd most likely look at what the total rings are for a few abalone, in our dataset,  with shell weight closest to 497 grams and calculate the average number of rings of the selected items.

Let's look at the 5 'nearest' shell_weights and calculate the average of the selected items.


```{r method_1, echo=TRUE}
options(scipen=999)
shell <- abalone$shell_weight
dists <- abs(shell - 0.497) # distances of shell_weight closest to 0.497
close5 <- order(dists)[1:5]

# The 5 closest distances to shell_weight of 0.497 are:
dists[close5]

# and the 5 corresponding closest shell_weights are:
abalone$shell_weight[close5]

# the total_rings for each of these 5 closest items are:
abalone$total_rings[close5]

# and the mean total_rings for the 5 closest shell weights is:
mean(abalone$total_rings[close5])
```

When we decided to look at the 5 closest shell weights, the decision to select the 5 closest shell weights as opposed to 20, was arbitrary. In Machine Learning (ML) k denotes the arbitrary number (5) we chose. These 5 are called the **k** nearest neigbours. So how do we decide what the best **k** is? 
There is no sure way to choose the best **k**. Various methods are used in practice that work well.
A rule of thumb derived from mathematical theory is:
$$k < \sqrt{n}$$
where $n$ is the number of items (rows) in your dataset.

#### The regtools package/kNN() function 

Now let's predict total_rings with the regtools package function kNN() again assuming we foraged an abalone in the shallows and recorded all the features' measurements.

The kNN(**x**, **y**, **newx**, **k**) function takes the following basic arguments:

* **x**: the X matrix for the the training set. It has to be a matrix because 'nearest-neigbour' distances between rows must be computed.
* **y**: the Y vector for the training set.
* **newx**: a vector of feature values for a new case or a matrix of vectors.
* **k**: the number of nearest neighbours we wish to use.

But first, convert the sex variable to a dummy since this is a regtools package requirement.

```{r first, echo=TRUE}
abalone <- abalone %>% mutate_if(is.character, as.factor)
aBalone <- factorsToDummies(abalone, omitLast = TRUE) # factorsToDummies() coerces the data.frame to a matrix array

# Look at the column names after converting the 'sex' feature to dummy. 
colnames(aBalone)

# Observe that the number of variables has increased from 9 to 10.
dim(aBalone)

# Create the X matrix to be the training set.
abalone.x <- aBalone[, 1:9]

# And the Y vector for the training set.
age <- aBalone[, 10]

# Now let's begin using kNN() by predicting the rings for one abalone (some random abalone).
knnout <- kNN(abalone.x, age, c(0, 0, 0.35, 0.39, 0.09, 0.46, 0.2, 0.11, 0.12), 5)

# Here we see the 5 positions (row numbers) of the predictions.
knnout$whichClosest

# The output below shows us the average of the 5 predictions. It is the regression estimate.
knnout$regests

# Below we access the total_rings that corresponds with the indexes shown above.
aBalone[c(2205, 3154, 1, 12, 3837), 10]

# And confirm the average.
mean(aBalone[c(2205, 3154, 1, 12, 3837), 10])
```

#### kNN() and prediction using the original dataset

Let's continue to demonstrate usage of the kNN() function by predicting the rings for the original dataset. To begin, let's set **k** = 5. 

```{r entire, echo=TRUE}
knnout <- kNNallK(abalone.x, age, abalone.x, 5, allK = TRUE)

# Here we see the structure of the new object which is a list.
str(knnout)

# The matrix below shows that we generate 60 sets of 4177 predictions. Each row has 60 nearest neighbour predictions. (Indicates which row we will find the closest estimate).
# Below we show the nearest neigbour predictions for only the first 3 rows of the prediction dataset. 
knnout$whichClosest[1:3, 1:5]

# See the predicted values (Y) using the original data. These are the regression estimates.
# Row 1 has all the predicted values for k=1, row 2 shows the predicted values for k = 2, etc.
knnout$regests[1:3, 1:5] 


# The first 5 real Y values are shown below
age[1:5] # the predicted values for k=1 is the same as this
```

Notice that the entries of the first row of the regression estimates (regests) are identical to the real values (age[1:5]). Because we set both **x** and **newx** to aBalone.x, we observe that the first element in the first column 'says' that the closest row in aBalone.x to the first row in aBalone.x is the first row in aBalone.x. So the closest data point is itself. This is called overfitting. And overfitting should be avoided like the plague. How do we achieve this? We leave out **k** = 1. The kNN() function has an argument that permits us to do that. The argument is called leave1out. See its use below.

#### Now use **k** = 60
We finally settle on a value for k that is less than $\sqrt{n}$. 60 is a little less than $\sqrt{n}$.

We also use the function that evaluates the prediction accuracy. The function is findOverallLoss().
For each value in Y (age), we take the absolute difference between the actual value and predicted values, then compute the average for those absolute differences to get the Mean Absolute Prediction Error (MAPE).

```{r leave1out, echo=TRUE}
# Here we run the function but exclude k = 1
knnout <- kNNallK(abalone.x, age, abalone.x, 60, allK = TRUE, leave1out = TRUE)

# Now see the predicted row positions for first 3 items.
knnout$whichClosest[1:3, 1:60]

# An see the regression estimates for the first 3 items
knnout$regests[1:3, 1:60]

# Below see the loss error for each k value. The Mean Absolute Prediction Error (MAPE)
findOverallLoss(knnout$regests, age)

# The optimal k is:
which.min(findOverallLoss(knnout$regests, age))

# And the minimum loss error is:
min(findOverallLoss(knnout$regests, age))

```

#### The partykit package

Here we will predict the number of rings using decision trees (DT). DT are basically flow charts. Like kNN, they look at the neighbourhood of the point to be predicted, only in a more sophisticated way.

So, to reiterate, the DT method sets up the prediction process as a flow chart. At the top of the tree we split the data into 2 parts. Then we split each of those parts into 2 further parts, etc,. An alternative name for this process is *recursive partitioning*.

First we dummify the data. The argument fullRank is used. The end result is that we produce 2 dummy variables for the 'sex' variable as opposed to 3.

```{r dummy, echo=TRUE, message=FALSE}
dmy <- dummyVars("~ .", data = abalone, fullRank = TRUE)
abalone <- data.frame(predict(dmy, newdata = abalone))
```


```{r DT, echo=TRUE, message=FALSE}
abba <- ctree(total_rings ~.,
              data = abalone,
              control = ctree_control(maxdepth = 3))
```

##### The plot

The plot (below) shows that the DT does take the form of a flow chart.
The plot says: For an abalone within a given level of shell_weight, sex, shucked_weight, etc, what value should we predict for total_rings? The graph shows the prediction procedure:

1. For our first cut in predicting the total rings, look to shell_weight. If it is less than or equal to 168 grams, go to the left branch of the flow chart, otherwise go right. WE have split Node 1 of the tree.
2. If you are on the right branch, continue to look at shell_weight. If the shell_weight is at most 374 grams, continue to look at shell_weight. And if the shell_weight is greater than 249 grams, select Node 12 where you will see that your total_rings prediction is approximately 11.
3. If you choose the left branch from Node 1 there will again be a decision based on shell_weight comparing it to 59 grams. If the shell_weight is greater than 59 grams we look to sex.I (infant) and wind up in either Node 7 or Node 8. If the shell_weight is less than 59 grams, we look again to shell_weight and compare it to 26 grams. If it is at most 26 grams we slect Node 4 else we select Node 5.


```{r the_plot, echo=TRUE, message=FALSE}
plot(abba)
```

Nodes 4, 5, 7, 8, 11, 12, 14, 15 are terminal nodes. They do not split. We can access these programmatically as seen below.

```{r terminal_nodes, echo=TRUE, message=FALSE}
# Terminal Nodes
nodeids(abba, terminal = TRUE)
```

In order to compute the predicted value for total_rings in say, Node 14, we would ordinarily use the **predict()** function. We can though examine the output object **abba**.

The display shows the number of original data points ending up in each terminal node, the mean squared prediction error for those points as well as the mean prediction value (Y) in each of the nodes. So, by example, Node 4has a prediction value of 4.458 total_rings and a mean squared error value of 123.3 for the 118 points in Node 4.

```{r output, echo=TRUE, message=FALSE}
# Printed version of the output.
abba
```

#### Results

The results for the prediction model are as seen below. See the results for the mean prediction value and median prediction value for each node.

```{r results_final, echo=TRUE, message=FALSE}
# First see the node terminals.
nodeIDs <- nodeids(abba, terminal = TRUE)

# Then the median Y for each node.
mdn <- function(yvals, weights) median(yvals)
predict_party(abba, id=nodeIDs, FUN = mdn)

# And finally the mean Y for each node
predict_party(abba, id=nodeIDs, FUN = function(yvals, weights) mean(yvals))

```

If you wish to see the predicted values for all the data points for an individual node then the code below applies. Select the individual terminal node (id) to observe the predicted results for the data points: 

```{r results, echo=TRUE, message=FALSE}
# The Y values in a given node. Select the node number and insert in the id argument.
f1 <- function(yvals, weights) c(yvals)
predict_party(abba, id=4, FUN = f1)
```

Or if you wish to see the median of a selected node use the following code:

```{r median_results, echo=TRUE, message=FALSE}
# The median of the Y values of a specific node.
mdn <- function(yvals, weights) median(yvals)
predict_party(abba, id=15, FUN = mdn)

```

Below you see the code to predict the total_rings of a new item.
```{r newx, echo=TRUE, message=FALSE}
# Here's how to predict the total_rings for a recently retrieved abalone.
newx <- data.frame(sex.I=0,sex.M=1,length=0.495,diameter=0.503,height=0.137, 
                   whole_weight=0.5347, shucked_weight=0.372, viscera_weight=0.301,
                   shell_weight=0.267)
predict(abba,newx,FUN=mdn)
```

#### Conclusion

This report presents the results of the prediction of the age, indicated by rings, of abalone by using the **kNN()** function of the **regtools** package, and the **ctree()** function of the **partykit** package. A loss function is presented in the **kNN()** function section and the mean squared prediction error is calculated for the points in each node.

As to impact, I am unsure, but I humbly assume that the approach could be useful to the marine and fisheries regulatory authorities in South Africa and possibly to the marine research communities.

There are obvious limitations. I think more effective Machine Learning algorithms should produce even better results. I am curious about the Random Forests function of the **partykit** package. Hopefully when I become better skilled in ML and ML mathematics I can apply alternative ML algorithms. I found the loss calculations not as intuitive as the RMSE loss calculation for the **kNN()** and **ctree()** functions.



